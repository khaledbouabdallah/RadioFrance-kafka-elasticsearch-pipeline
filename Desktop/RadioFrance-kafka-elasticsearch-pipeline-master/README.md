# ğŸ™ï¸ Radio France Streaming Analytics Pipeline with Kafka, Spark and Elastic Stack

A real-time data pipeline for collecting, processing, analyzing, and
visualizing live broadcasts from Radio France stations.

This project demonstrates a complete modern streaming analytics
architecture using Big Data technologies.

---

## ğŸ¯ Features

### ğŸ”„ Real-time Data Collection

- Automatic polling of Radio France GraphQL API every 5 minutes\
- 30+ stations covered (France Inter, France Culture, France Info,
  France Musique, FIP, Mouv', local France Bleu stations)\
- Enriched data: geolocation, broadcast themes\
- Kafka publishing: structured JSON messages to `radiofrance-live`
  topic

### âš¡ Data Processing

- Logstash enrichment and transformation\
- Automatic geocoding for local stations\
- Theme extraction and categorization\
- Elasticsearch indexing optimized for search

### ğŸ§  Distributed Analytics

- Apache Spark distributed computations\
- Aggregations by station, hour, theme\
- Real-time broadcast calculations\
- Results persisted to Elasticsearch

### ğŸ“Š Visualization

- Kibana dashboards with auto-refresh\
- Raw vs analytics indices\
- Geographic mapping for France Bleu stations

---

### Data Flow

1.  **Collection:** Python â†’ Radio France API â†’ Kafka\
2.  **Processing:** Kafka â†’ Logstash â†’ Elasticsearch (raw)\
3.  **Analytics:** Elasticsearch â†’ Spark â†’ Elasticsearch (results)\
4.  **Visualization:** Elasticsearch â†’ Kibana

---

## ğŸ“ Project Structure

    RadioFrance-kafka-elasticsearch-pipeline/
    â”œâ”€â”€ ğŸ“„ docker-compose.yml              # Docker services configuration
    â”œâ”€â”€ ğŸ“„ .env                            # Environment variables template
    â”œâ”€â”€ ğŸ“„ README.md                       # This documentation
    â”œâ”€â”€ ğŸ“„ RADIOFRANCE_API.md              # How to use the Radio France Open API
    â”œâ”€â”€ ğŸ“ api-collector/                  # Data collection service
    â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile                  # Collector Docker image
    â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
    â”‚   â””â”€â”€ ğŸ“„ radiofrance_realtime_collector.py  # Main collection script
    â”œâ”€â”€ ğŸ“ elasticsearch/                  # Elasticsearch configuration
    â”‚   â””â”€â”€ ğŸ“ mappings/                   # Index mappings
    â”‚       â”œâ”€â”€ ğŸ“„ radiofrance-mapping.json        # Raw data mapping
    â”‚       â””â”€â”€ ğŸ“„ radiofrance-analytics-mapping.json  # Analytics results mapping
            ğŸ“ queries/                    # Queries to index
    â”‚       â”œâ”€â”€ ğŸ“„ aggs.json               # Aggregation query
    â”‚       â””â”€â”€ ğŸ“„ fuzzy.json              # Fuzziness query
            â””â”€â”€ ğŸ“„ n-gram.json             # N-gram query
            â””â”€â”€ ğŸ“„ temporal-serie.json     # Time serie query
            â””â”€â”€ ğŸ“„ textual.json            # Textual query
    â”œâ”€â”€ ğŸ“ logstash/                       # Logstash configuration
    â”‚   â””â”€â”€ ğŸ“ pipeline/
    â”‚       â””â”€â”€ ğŸ“„ radiofrance-live.conf   # Processing pipeline
    â””â”€â”€ ğŸ“ spark/                          # Spark jobs and configuration
        â”œâ”€â”€ ğŸ“ config/
        â”‚   â””â”€â”€ ğŸ“„ spark_config.py         # Spark session configuration
        â””â”€â”€ ğŸ“ jobs/
            â””â”€â”€ ğŸ“„ spark_corrected_final.py  # Main analytics job (final version)

---

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop 20.10+\
- 4GB+ RAM allocated to Docker\
- Git
- Radio France API Key (for testing)

### Installation

```bash
git clone https://github.com/khaledbouabdallah/RadioFrance-kafka-elasticsearch-pipeline.git
cd RadioFrance-kafka-elasticsearch-pipeline
# Edit the .env file with your parameters
```

Start services:

```bash
# Launch all services
docker-compose up -d
# Verify all services are running
docker-compose ps
```

---

## âš™ï¸ Configuration

Example `.env`:

```env
RADIOFRANCE_API_KEY=your_api_key_here
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC=radiofrance-live
POLL_INTERVAL=300
ELASTICSEARCH_HOST=http://elasticsearch:9200
SPARK_MASTER=spark://spark-master:7077
```

---

## ğŸ”§ Detailed Services

- **Zookeeper** -- Kafka coordination\
- **Kafka** -- Streaming broker\
- **Elasticsearch** -- Data storage and search\
- **Kibana** -- Visualization\
- **Logstash** -- Data processing\
- **API Collector** -- Data ingestion\
- **Spark Cluster** -- Distributed analytics

---

## ğŸŒ Access interfaces

- Kibana : http://localhost:15601
- Spark Master UI : http://localhost:18080
- Elasticsearch : http://localhost:19200
- Spark Worker UI : http://localhost:18081

## ğŸ“Š Spark Analytics Job

Manual execution:

```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.elasticsearch:elasticsearch-spark-30_2.12:8.12.0 \
  /opt/spark/jobs/spark_corrected_final.py
```

What the Job Does :

- Loads data from Elasticsearch (index radiofrance-live-\*)

- Cleans and transforms the data

- Calculates statistics:

      +   Total broadcasts per station

      +   Average themes per broadcast

      +   Number of available podcasts

      +   Temporal distribution of broadcasts

- Saves results to Elasticsearch (index radiofrance-analytics\*)

---

## ğŸ“ˆ Kibana Visualization

1.  Create Data Views:
    - `radiofrance-live*`
    - `radiofrance-analytics*`
2.  Build visualizations:
    - Maps\
    - Lens charts\
    - Heatmaps\
    - Tag Cloud\
    - Data Tables
3.  Create dashboard:
    - Auto-refresh: 30s\
    - Time range: Last 1 hour

---

## ğŸ§ª Testing and Validation

### Docker

```bash
# Verify all services are running
docker-compose ps
# Show logs for a specific service
docker-compose logs -f api-collector
```

### Kafka

```bash
# List Kafka topics
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

### Elasticsearch

```bash
# List all indices
curl http://localhost:19200/_cat/indices?v
# Count Radio France documents
curl http://localhost:19200/radiofrance*/_count
# See a sample document
curl http://localhost:19200/radiofrance-live*/_search?size=1&pretty
```

### Spark

```bash
# Run a simple test
docker exec spark-master /opt/spark/bin/spark-submit --version
```

---

## ğŸ” Troubleshooting

### Port Already in Use

Modify ports in `docker-compose.yml`.

### No Data in Kibana

- Check time range\
- Verify Data Views\
- Confirm Elasticsearch indices

### Reset Environment

```bash
docker-compose down -v
docker-compose up -d --build
```

---

## ğŸ‘¥ Contributing

1.  Fork the repository\
2.  Create a feature branch\
3.  Commit changes\
4.  Push and open a Pull Request

### Code Standards

- Python: PEP 8\
- Docker best practices\
- Clear Markdown documentation

---

**Author:** Mouad TAHIRI - Khaled BOUABDALLAH
 
lun.  9 fÃ©vr. 2026 22:04:09
